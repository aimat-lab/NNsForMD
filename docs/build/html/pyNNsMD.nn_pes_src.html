
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>pyNNsMD.nn_pes_src package &#8212; NNsForMD 0.5.0 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="pyNNsMD package" href="pyNNsMD.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="pyNNsMD.html" title="pyNNsMD package"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">NNsForMD 0.5.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="modules.html" >pyNNsMD</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="pyNNsMD.html" accesskey="U">pyNNsMD package</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">pyNNsMD.nn_pes_src package</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="pynnsmd-nn-pes-src-package">
<h1>pyNNsMD.nn_pes_src package<a class="headerlink" href="#pynnsmd-nn-pes-src-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.activ">
<span id="pynnsmd-nn-pes-src-activ-module"></span><h2>pyNNsMD.nn_pes_src.activ module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.activ" title="Permalink to this headline">¶</a></h2>
<p>Smooth activation functions for tensorflow.keras.</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.activ.identify_keras_activation">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.activ.</code><code class="sig-name descname">identify_keras_activation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">instr</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">beta</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.activ.identify_keras_activation" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify ativation function by string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>instr</strong> (<em>str</em>) – Name of function.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – Alpha Paramter. The default is None.</p></li>
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>optional</em>) – Beta Parameter. The default is None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>activ</strong> – Activation function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>fun</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.activ.leaky_softplus">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.activ.</code><code class="sig-name descname">leaky_softplus</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.3</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.activ.leaky_softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Leaky softplus activation function similar to leakyRELU but smooth.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – Leaking slope. The default is 0.3.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>lambda function of x.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>func</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.activ.shifted_sofplus">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.activ.</code><code class="sig-name descname">shifted_sofplus</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.activ.shifted_sofplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Softplus function from tf.keras shifted downwards.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tf.tensor</em>) – Activation input.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Activation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.callbacks">
<span id="pynnsmd-nn-pes-src-callbacks-module"></span><h2>pyNNsMD.nn_pes_src.callbacks module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.callbacks" title="Permalink to this headline">¶</a></h2>
<p>Callbacks for learning rate schedules.</p>
<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.callbacks.EarlyStopping">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.callbacks.</code><code class="sig-name descname">EarlyStopping</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">minutes</span><span class="o">=</span><span class="default_value">inf</span></em>, <em class="sig-param"><span class="n">epochs</span><span class="o">=</span><span class="default_value">inf</span></em>, <em class="sig-param"><span class="n">learning_rate</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">epostep</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">monitor</span><span class="o">=</span><span class="default_value">'val_loss'</span></em>, <em class="sig-param"><span class="n">min_delta</span><span class="o">=</span><span class="default_value">1e-05</span></em>, <em class="sig-param"><span class="n">patience</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">minEpoch</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">factor</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">min_lr</span><span class="o">=</span><span class="default_value">1e-06</span></em>, <em class="sig-param"><span class="n">store_weights</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">restore_weights_on_lr_decay</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.callbacks.EarlyStopping" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.callbacks.Callback</span></code></p>
<p>This Callback does basic monitoring of the learning process and provides functionality such as learning rate decay
and early stopping with custom logic as opposed to the callbacks provided by Keras by default which are generic.
By André Eberhard
<a class="reference external" href="https://github.com/patchmeifyoucan">https://github.com/patchmeifyoucan</a></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.callbacks.EarlyStopping.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span></em>, <em class="sig-param"><span class="n">logs</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.callbacks.EarlyStopping.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of an epoch.</p>
<p>Subclasses should override for any actions to run. This function should only
be called during TRAIN mode.</p>
<dl>
<dt>Arguments:</dt><dd><p>epoch: Integer, index of epoch.
logs: Dict, metric results for this training epoch, and for the</p>
<blockquote>
<div><p>validation epoch if validation is performed. Validation result keys
are prefixed with <cite>val_</cite>.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.callbacks.EarlyStopping.on_train_begin">
<code class="sig-name descname">on_train_begin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logs</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.callbacks.EarlyStopping.on_train_begin" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of training.</p>
<p>Subclasses should override for any actions to run.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>logs: Dict. Currently no data is passed to this argument for this method</dt><dd><p>but that may change in the future.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.callbacks.EarlyStopping.on_train_end">
<code class="sig-name descname">on_train_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logs</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.callbacks.EarlyStopping.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of training.</p>
<p>Subclasses should override for any actions to run.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>logs: Dict. Currently the output of the last call to <cite>on_epoch_end()</cite></dt><dd><p>is passed to this argument for this method but that may change in
the future.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.callbacks.lr_exp_reduction">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.callbacks.</code><code class="sig-name descname">lr_exp_reduction</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr_start</span></em>, <em class="sig-param"><span class="n">epomin</span></em>, <em class="sig-param"><span class="n">epostep</span></em>, <em class="sig-param"><span class="n">facred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.callbacks.lr_exp_reduction" title="Permalink to this definition">¶</a></dt>
<dd><p>Make learning rate schedule function for exponential reduction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr_start</strong> (<em>float</em>) – Learning rate to start with.</p></li>
<li><p><strong>epomin</strong> (<em>float</em>) – Minimum number of epochs to keep learning rate constant.</p></li>
<li><p><strong>epostep</strong> (<em>float</em>) – The epochs to divide factor by.</p></li>
<li><p><strong>facred</strong> (<em>float</em>) – Reduce learning rate by factor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Function to use with LearningRateScheduler.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>func</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>lr_schedule_steps = tf.keras.callbacks.LearningRateScheduler(lr_exp_reduction)</p>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.callbacks.lr_lin_reduction">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.callbacks.</code><code class="sig-name descname">lr_lin_reduction</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learning_rate_start</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">learning_rate_stop</span><span class="o">=</span><span class="default_value">1e-05</span></em>, <em class="sig-param"><span class="n">epo</span><span class="o">=</span><span class="default_value">10000</span></em>, <em class="sig-param"><span class="n">epomin</span><span class="o">=</span><span class="default_value">1000</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.callbacks.lr_lin_reduction" title="Permalink to this definition">¶</a></dt>
<dd><p>Make learning rate schedule function for linear reduction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate_start</strong> (<em>float</em><em>, </em><em>optional</em>) – Learning rate to start with. The default is 1e-3.</p></li>
<li><p><strong>learning_rate_stop</strong> (<em>float</em><em>, </em><em>optional</em>) – Final learning rate at the end of epo. The default is 1e-5.</p></li>
<li><p><strong>epo</strong> (<em>int</em><em>, </em><em>optional</em>) – Total number of epochs to reduce learning rate towards. The default is 10000.</p></li>
<li><p><strong>epomin</strong> (<em>int</em><em>, </em><em>optional</em>) – Minimum number of epochs at beginning to leave learning rate constant. The default is 1000.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Function to use with LearningRateScheduler.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>func</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>lr_schedule_steps = tf.keras.callbacks.LearningRateScheduler(lr_lin_reduction)</p>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.callbacks.lr_step_reduction">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.callbacks.</code><code class="sig-name descname">lr_step_reduction</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learnrate_steps</span><span class="o">=</span><span class="default_value">[0.001, 0.0001, 1e-05]</span></em>, <em class="sig-param"><span class="n">learnrate_epochs</span><span class="o">=</span><span class="default_value">[500, 1000, 5000]</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.callbacks.lr_step_reduction" title="Permalink to this definition">¶</a></dt>
<dd><p>Make learning rate schedule function for step reduction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learnrate_steps</strong> (<em>list</em><em>, </em><em>optional</em>) – List of learning rates for each step. The default is [1e-3,1e-4,1e-5].</p></li>
<li><p><strong>learnrate_epochs</strong> (<em>list</em><em>, </em><em>optional</em>) – The length of each step to keep learning rate. The default is [500,1000,5000].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Function that can be used with LearningRateScheduler.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>func</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>lr_schedule_steps = tf.keras.callbacks.LearningRateScheduler(lr_step_reduction)</p>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.data">
<span id="pynnsmd-nn-pes-src-data-module"></span><h2>pyNNsMD.nn_pes_src.data module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.data" title="Permalink to this headline">¶</a></h2>
<p>Tools for handling and storing data. For example to save to folder etc.</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.data.datalist_make_random_shuffle">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.data.</code><code class="sig-name descname">datalist_make_random_shuffle</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">datalist</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.data.datalist_make_random_shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Shuffle a list od data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>datalist</strong> (<em>list</em>) – List of numpy arrays of same length (axis=0).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>allind</strong> (<em>np.array</em>) – Index assignment of the shuffle.</p></li>
<li><p><strong>outlist</strong> (<em>list</em>) – List of the shuffled data.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.data.index_data_in_y_dict">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.data.</code><code class="sig-name descname">index_data_in_y_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">ind</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.data.index_data_in_y_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Index np.arrays as array[index] in the nested y_dict used in pes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>dict</em>) – Dcitionary of y-values as y={‘energy_gradients’ : [np.array,np.array], ‘NAC’ : np.array}.</p></li>
<li><p><strong>ind</strong> (<em>np.array</em>) – Index array.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_out</strong> – Same y_dict with its data as data[index].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.data.index_make_random_shuffle">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.data.</code><code class="sig-name descname">index_make_random_shuffle</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.data.index_make_random_shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Shuffle indexarray.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np.array</em>) – Index to shuffle.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Shuffled index.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.data.merge_data_in_chunks">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.data.</code><code class="sig-name descname">merge_data_in_chunks</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data1</span></em>, <em class="sig-param"><span class="n">data2</span></em>, <em class="sig-param"><span class="n">split_size</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.data.merge_data_in_chunks" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge data in chunks of split-size. Goal is to keep validation k-splits for fit.</p>
<p>Idea: [a+a+a] + [b+b+b] = [(a+b)+(a+b)+(a+b)] and NOT [a+a+a+b+b+b].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data1</strong> (<em>np.array</em>) – Data to merge.</p></li>
<li><p><strong>data2</strong> (<em>np.array</em>) – Data to merge.</p></li>
<li><p><strong>split_size</strong> (<em>float</em>) – Relative size of junks 0 &lt; split_size &lt; 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Merged data.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.data.model_save_data_to_folder">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.data.</code><code class="sig-name descname">model_save_data_to_folder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">target_model</span></em>, <em class="sig-param"><span class="n">mod_dir</span></em>, <em class="sig-param"><span class="n">random_shuffle</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.data.model_save_data_to_folder" title="Permalink to this definition">¶</a></dt>
<dd><p>Save Data to model folder. Always dumps data_x and data_y as pickle.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>np.array</em>) – Coordinates as x-data.</p></li>
<li><p><strong>y</strong> (<em>list</em><em>,</em><em>np.array</em>) – A possible list of np.arrays for y-values. Energy, Gradients, NAC etc.</p></li>
<li><p><strong>target_model</strong> (<em>str</em>) – Name of the Model to save data for.</p></li>
<li><p><strong>mod_dir</strong> (<em>str</em>) – Path of model directory.</p></li>
<li><p><strong>random_shuffle</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to shuffle data before save. The default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.data.split_validation_training_index">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.data.</code><code class="sig-name descname">split_validation_training_index</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">allind</span></em>, <em class="sig-param"><span class="n">splitsize</span></em>, <em class="sig-param"><span class="n">do_offset</span></em>, <em class="sig-param"><span class="n">offset_steps</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.data.split_validation_training_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a train-validation split for indexarray. Validation set is taken from beginning with possible offset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>allind</strong> (<em>np.array</em>) – Indexlist for full dataset of same length.</p></li>
<li><p><strong>splitsize</strong> (<em>int</em>) – Total number of validation samples to take.</p></li>
<li><p><strong>do_offset</strong> (<em>bool</em>) – Whether to take validation set not from beginnig but with offset.</p></li>
<li><p><strong>offset_steps</strong> (<em>int</em>) – Number of validation sizes offseted from the beginning to start to take validation set.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>i_train</strong> (<em>np.array</em>) – Training indices.</p></li>
<li><p><strong>i_val</strong> (<em>np.arry</em>) – Validation indices.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.device">
<span id="pynnsmd-nn-pes-src-device-module"></span><h2>pyNNsMD.nn_pes_src.device module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.device" title="Permalink to this headline">¶</a></h2>
<p>Sets the devices for training scripts.</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.device.set_gpu">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.device.</code><code class="sig-name descname">set_gpu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gpu_ids_list</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.device.set_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the visible devices from a list of GPUs. Used to assign a process to a separate GPU.
Also very important is to restrict memeory growth since a single tensorfow process will allocate almost all
GPU memory, so two fits can not run on same GPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gpu_ids_list</strong> (<em>list</em>) – Device list.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.fit">
<span id="pynnsmd-nn-pes-src-fit-module"></span><h2>pyNNsMD.nn_pes_src.fit module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.fit" title="Permalink to this headline">¶</a></h2>
<p>Main interface to start <a href="#id35"><span class="problematic" id="id36">training_</span></a>??.py scripts in parallel. This can be solved in many different ways.
Possible are server solutions with slurm and MPI. Here only python subprocess are started to local machine.
The training scripts are supposed to read all necessary information from folder.
NOTE: Path information of folder and training scripts as well as os info are made fetchable but could fail in certain
circumstances.</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.fit.fit_model_energy_gradient">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.fit.</code><code class="sig-name descname">fit_model_energy_gradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">filepath</span></em>, <em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">m</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.fit.fit_model_energy_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the training script in subprocess.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>i</strong> (<em>int</em>) – Index of model.</p></li>
<li><p><strong>filepath</strong> (<em>str</em>) – Filepath to model.</p></li>
<li><p><strong>g</strong> (<em>int</em>) – GPU index to use.</p></li>
<li><p><strong>m</strong> (<em>str</em>) – Fitmode.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.fit.fit_model_energy_gradient_async">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.fit.</code><code class="sig-name descname">fit_model_energy_gradient_async</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">filepath</span></em>, <em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">m</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.fit.fit_model_energy_gradient_async" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the training script in subprocess.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>i</strong> (<em>int</em>) – Index of model.</p></li>
<li><p><strong>filepath</strong> (<em>str</em>) – Filepath to model.</p></li>
<li><p><strong>g</strong> (<em>int</em>) – GPU index to use.</p></li>
<li><p><strong>m</strong> (<em>str</em>) – Fitmode.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>subprocess.Popen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.fit.fit_model_get_python_cmd_os">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.fit.</code><code class="sig-name descname">fit_model_get_python_cmd_os</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.fit.fit_model_get_python_cmd_os" title="Permalink to this definition">¶</a></dt>
<dd><p>Return proper commandline command for pyhton depending on os.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python command either python or pyhton3.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.fit.fit_model_nac">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.fit.</code><code class="sig-name descname">fit_model_nac</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">filepath</span></em>, <em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">m</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.fit.fit_model_nac" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the training script in subprocess.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>i</strong> (<em>int</em>) – Index of model.</p></li>
<li><p><strong>filepath</strong> (<em>str</em>) – Filepath to model.</p></li>
<li><p><strong>g</strong> (<em>int</em>) – GPU index to use.</p></li>
<li><p><strong>m</strong> (<em>str</em>) – Fitmode.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.fit.fit_model_nac_async">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.fit.</code><code class="sig-name descname">fit_model_nac_async</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">filepath</span></em>, <em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">m</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.fit.fit_model_nac_async" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the training script in subprocess.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>i</strong> (<em>int</em>) – Index of model.</p></li>
<li><p><strong>filepath</strong> (<em>str</em>) – Filepath to model.</p></li>
<li><p><strong>g</strong> (<em>int</em>) – GPU index to use.</p></li>
<li><p><strong>m</strong> (<em>str</em>) – Fitmode.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>subprocess.Popen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.fit.get_path_for_fit_script">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.fit.</code><code class="sig-name descname">get_path_for_fit_script</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.fit.get_path_for_fit_script" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to find the path of training scripts.
For now they are expected to be in the same folder as calling .py script.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>filepath</strong> – Filepath pointing to training scripts.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.hyper">
<span id="pynnsmd-nn-pes-src-hyper-module"></span><h2>pyNNsMD.nn_pes_src.hyper module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.hyper" title="Permalink to this headline">¶</a></h2>
<p>Default hyperparameters</p>
</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.layers">
<span id="pynnsmd-nn-pes-src-layers-module"></span><h2>pyNNsMD.nn_pes_src.layers module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.layers" title="Permalink to this headline">¶</a></h2>
<p>Created on Tue Jul  7 12:30:58 2020</p>
<p>&#64;author: Patrick</p>
<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.Angles">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">Angles</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.Angles" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.Angles.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.Angles.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.Angles.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.Angles.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.Angles.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.Angles.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.Dihydral">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">Dihydral</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.Dihydral" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.Dihydral.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.Dihydral.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.Dihydral.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.Dihydral.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.Dihydral.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.Dihydral.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.EmptyGradient">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">EmptyGradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.EmptyGradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.EmptyGradient.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.EmptyGradient.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.EmptyGradient.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.EmptyGradient.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.EmptyGradient.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.EmptyGradient.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.EnergyGradient">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">EnergyGradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.EnergyGradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.EnergyGradient.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.EnergyGradient.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.EnergyGradient.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.EnergyGradient.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id7"><span class="problematic" id="id8">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.EnergyGradient.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.EnergyGradient.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.InverseDistance">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">InverseDistance</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.InverseDistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.InverseDistance.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.InverseDistance.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.InverseDistance.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.InverseDistance.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id9"><span class="problematic" id="id10">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.InverseDistance.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.InverseDistance.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.MLP">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">MLP</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.MLP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.MLP.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.MLP.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.MLP.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">training</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.MLP.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id11"><span class="problematic" id="id12">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.MLP.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.MLP.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.NACGradient">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">NACGradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.NACGradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.NACGradient.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.NACGradient.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.NACGradient.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.NACGradient.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id13"><span class="problematic" id="id14">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.NACGradient.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.NACGradient.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.PropagateEnergyGradient">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">PropagateEnergyGradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.PropagateEnergyGradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.PropagateEnergyGradient.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.PropagateEnergyGradient.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.PropagateEnergyGradient.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.PropagateEnergyGradient.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id15"><span class="problematic" id="id16">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.PropagateEnergyGradient.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.PropagateEnergyGradient.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.PropagateNACGradient">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">PropagateNACGradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.PropagateNACGradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.PropagateNACGradient.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.PropagateNACGradient.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.PropagateNACGradient.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.PropagateNACGradient.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id17"><span class="problematic" id="id18">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.PropagateNACGradient.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.PropagateNACGradient.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.layers.RevertStandardize">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.layers.</code><code class="sig-name descname">RevertStandardize</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.RevertStandardize" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.RevertStandardize.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.RevertStandardize.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.RevertStandardize.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.RevertStandardize.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id19"><span class="problematic" id="id20">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.layers.RevertStandardize.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.layers.RevertStandardize.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.legacy">
<span id="pynnsmd-nn-pes-src-legacy-module"></span><h2>pyNNsMD.nn_pes_src.legacy module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.legacy" title="Permalink to this headline">¶</a></h2>
<p>Old unused functions. May not be working anymore.</p>
<p>&#64;author: Patrick</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.legacy.compute_feature_derivative">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.legacy.</code><code class="sig-name descname">compute_feature_derivative</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">hyper</span></em>, <em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.legacy.compute_feature_derivative" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.loss">
<span id="pynnsmd-nn-pes-src-loss-module"></span><h2>pyNNsMD.nn_pes_src.loss module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.loss" title="Permalink to this headline">¶</a></h2>
<p>Functions for loss.</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.loss.get_lr_metric">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.loss.</code><code class="sig-name descname">get_lr_metric</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.loss.get_lr_metric" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtian learning rate from optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer</strong> (<em>tf.kears.optimizer</em>) – Optimizer used for training.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>learning rate.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.loss.merge_hist">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.loss.</code><code class="sig-name descname">merge_hist</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hist1</span></em>, <em class="sig-param"><span class="n">hist2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.loss.merge_hist" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge two hist-dicts</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hist1</strong> (<em>dict</em>) – Hist dict from fit.</p></li>
<li><p><strong>hist2</strong> (<em>dict</em>) – Hist dict from fit.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>outhist</strong> – hist1 + hist2.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.loss.nac_loss">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.loss.</code><code class="sig-name descname">nac_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.loss.nac_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>A phaseless loss for the NAC prediction. Needs to be adapted for multiple states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>tf.tensor</em>) – True y-values.</p></li>
<li><p><strong>y_pred</strong> (<em>tf.tensor</em>) – Predicted y-values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Phaseindependent MSE.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.loss.r2_metric">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.loss.</code><code class="sig-name descname">r2_metric</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.loss.r2_metric" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute r2 metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>tf.tensor</em>) – True y-values.</p></li>
<li><p><strong>y_pred</strong> (<em>tf.tensor</em>) – Predicted y-values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>r2 metric.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.models_eg">
<span id="pynnsmd-nn-pes-src-models-eg-module"></span><h2>pyNNsMD.nn_pes_src.models_eg module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.models_eg" title="Permalink to this headline">¶</a></h2>
<p>Tensorflow keras model definitions for energy and gradient.</p>
<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.models_eg.EnergyModel">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_eg.</code><code class="sig-name descname">EnergyModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_eg.EnergyModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.training.Model</span></code></p>
<p>Subclassed tf.keras.model for energy/gradient which outputs both energy and gradient from coordinates.
This is not used for fitting, only for prediction as for fitting a feature-precomputed model is used instead.
The model is supposed to be saved and exported for MD code.</p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_eg.EnergyModel.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">training</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_eg.EnergyModel.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls the model on new inputs.</p>
<p>In this case <cite>call</cite> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<dl>
<dt>Arguments:</dt><dd><p>inputs: A tensor or list of tensors.
training: Boolean or boolean scalar tensor, indicating whether to run</p>
<blockquote>
<div><p>the <cite>Network</cite> in training mode or inference mode.</p>
</div></blockquote>
<dl class="simple">
<dt>mask: A mask or list of masks. A mask can be</dt><dd><p>either a tensor or None (no mask).</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.models_eg.EnergyModelPrecomputed">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_eg.</code><code class="sig-name descname">EnergyModelPrecomputed</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_eg.EnergyModelPrecomputed" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.training.Model</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_eg.EnergyModelPrecomputed.predict_step">
<code class="sig-name descname">predict_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_eg.EnergyModelPrecomputed.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one inference step.</p>
<p>This method can be overridden to support custom inference logic.
This method is called by <cite>Model.make_predict_function</cite>.</p>
<p>This method should contain the mathemetical logic for one step of inference.
This typically includes the forward pass.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_predict_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id21"><span class="problematic" id="id22">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>The result of one inference step, typically the output of calling the
<cite>Model</cite> on data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_eg.EnergyModelPrecomputed.test_step">
<code class="sig-name descname">test_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_eg.EnergyModelPrecomputed.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one evaluation step.</p>
<p>This method can be overridden to support custom evaluation logic.
This method is called by <cite>Model.make_test_function</cite>.</p>
<p>This function should contain the mathemetical logic for one step of
evaluation.
This typically includes the forward pass, loss calculation, and metrics
updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_test_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id23"><span class="problematic" id="id24">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_eg.EnergyModelPrecomputed.train_step">
<code class="sig-name descname">train_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_eg.EnergyModelPrecomputed.train_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <cite>Model.make_train_function</cite>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_train_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id25"><span class="problematic" id="id26">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned. Example:
<cite>{‘loss’: 0.2, ‘accuracy’: 0.7}</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.models_eg.create_model_energy_gradient_precomputed">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_eg.</code><code class="sig-name descname">create_model_energy_gradient_precomputed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hyper</span><span class="o">=</span><span class="default_value">{'general': {'info': '', 'main_dir': '', 'model_dir': '', 'model_type': 'energy_gradient', 'pyr_version': None}, 'model': {'Depth': 3, 'activ': 'leaky_softplus', 'activ_alpha': 0.03, 'angle_index': [], 'angle_mean': 0, 'angle_std': 1, 'atoms': 2, 'dihyd_index': [], 'dihyd_mean': 0, 'dihyd_std': 1, 'dropout': 0.005, 'invd_index': [], 'invd_mean': 0, 'invd_std': 1, 'learning_rate_compile': 0.001, 'loss_weights': [1, 10], 'nn_size': 1000, 'reg_l1': 1e-05, 'reg_l2': 1e-05, 'states': 1, 'use_bond_angles': False, 'use_dihyd_angles': False, 'use_dropout': False, 'use_invdist': True, 'use_reg_activ': None, 'use_reg_bias': None, 'use_reg_weight': None, 'y_energy_mean': 0, 'y_energy_std': 1, 'y_energy_unit_conv': 27.21138624598853, 'y_gradient_unit_conv': 51.42206747624915}, 'plots': {'unit_energy': 'eV', 'unit_gradient': 'eV/A'}, 'predict': {'batch_size_predict': 265, 'try_predict_hessian': False}, 'resample': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': False, 'val_split': 0.05}, 'retraining': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 2000, 'epoch_step_reduction': [1000, 500, 250], 'epomin': 1000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': False, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}, 'training': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_eg.create_model_energy_gradient_precomputed" title="Permalink to this definition">¶</a></dt>
<dd><p>Full Model y = model(feat) with feat=[f,df/dx] features and its derivative to coordinates x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyper</strong> (<em>dict</em><em>, </em><em>optional</em>) – Hyper dictionary. The default is hyper_model_energy_gradient.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>model</strong> – tf.keras model with coordinate input.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.model</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.models_feat">
<span id="pynnsmd-nn-pes-src-models-feat-module"></span><h2>pyNNsMD.nn_pes_src.models_feat module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.models_feat" title="Permalink to this headline">¶</a></h2>
<p>Tensorflow keras model definitions for features and descriptors.</p>
<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.models_feat.FeatureModel">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_feat.</code><code class="sig-name descname">FeatureModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_feat.FeatureModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.training.Model</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_feat.FeatureModel.predict_in_chunks">
<code class="sig-name descname">predict_in_chunks</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_feat.FeatureModel.predict_in_chunks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_feat.FeatureModel.predict_step">
<code class="sig-name descname">predict_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_feat.FeatureModel.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one inference step.</p>
<p>This method can be overridden to support custom inference logic.
This method is called by <cite>Model.make_predict_function</cite>.</p>
<p>This method should contain the mathemetical logic for one step of inference.
This typically includes the forward pass.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_predict_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id27"><span class="problematic" id="id28">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>The result of one inference step, typically the output of calling the
<cite>Model</cite> on data.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.models_feat.create_feature_models">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_feat.</code><code class="sig-name descname">create_feature_models</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hyper</span></em>, <em class="sig-param"><span class="n">model_name</span><span class="o">=</span><span class="default_value">'feat'</span></em>, <em class="sig-param"><span class="n">run_eagerly</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_feat.create_feature_models" title="Permalink to this definition">¶</a></dt>
<dd><p>Model to precompute features feat = model(x)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hyper</strong> (<em>dict</em>) – Hyper dictionary.</p></li>
<li><p><strong>run_eagerly</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to run eagerly.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>model</strong> – tf.keras model with coordinate input.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.model</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.models_nac">
<span id="pynnsmd-nn-pes-src-models-nac-module"></span><h2>pyNNsMD.nn_pes_src.models_nac module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.models_nac" title="Permalink to this headline">¶</a></h2>
<p>Tensorflow keras model definitions for NAC.</p>
<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.models_nac.NACModel">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_nac.</code><code class="sig-name descname">NACModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_nac.NACModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.training.Model</span></code></p>
<p>Subclassed tf.keras.model for NACs which outputs NACs from coordinates.
This is not used for fitting, only for prediction as for fitting a feature-precomputed model is used instead.
The model is supposed to be saved and exported.</p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_nac.NACModel.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">training</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_nac.NACModel.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls the model on new inputs.</p>
<p>In this case <cite>call</cite> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<dl>
<dt>Arguments:</dt><dd><p>inputs: A tensor or list of tensors.
training: Boolean or boolean scalar tensor, indicating whether to run</p>
<blockquote>
<div><p>the <cite>Network</cite> in training mode or inference mode.</p>
</div></blockquote>
<dl class="simple">
<dt>mask: A mask or list of masks. A mask can be</dt><dd><p>either a tensor or None (no mask).</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pyNNsMD.nn_pes_src.models_nac.NACModelPrecomputed">
<em class="property">class </em><code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_nac.</code><code class="sig-name descname">NACModelPrecomputed</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_nac.NACModelPrecomputed" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.training.Model</span></code></p>
<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_nac.NACModelPrecomputed.predict_step">
<code class="sig-name descname">predict_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_nac.NACModelPrecomputed.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one inference step.</p>
<p>This method can be overridden to support custom inference logic.
This method is called by <cite>Model.make_predict_function</cite>.</p>
<p>This method should contain the mathemetical logic for one step of inference.
This typically includes the forward pass.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_predict_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id29"><span class="problematic" id="id30">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>The result of one inference step, typically the output of calling the
<cite>Model</cite> on data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_nac.NACModelPrecomputed.test_step">
<code class="sig-name descname">test_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_nac.NACModelPrecomputed.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one evaluation step.</p>
<p>This method can be overridden to support custom evaluation logic.
This method is called by <cite>Model.make_test_function</cite>.</p>
<p>This function should contain the mathemetical logic for one step of
evaluation.
This typically includes the forward pass, loss calculation, and metrics
updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_test_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id31"><span class="problematic" id="id32">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pyNNsMD.nn_pes_src.models_nac.NACModelPrecomputed.train_step">
<code class="sig-name descname">train_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_nac.NACModelPrecomputed.train_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <cite>Model.make_train_function</cite>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_train_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id33"><span class="problematic" id="id34">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned. Example:
<cite>{‘loss’: 0.2, ‘accuracy’: 0.7}</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.models_nac.create_model_nac">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_nac.</code><code class="sig-name descname">create_model_nac</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hyper</span><span class="o">=</span><span class="default_value">{'general': {'info': '', 'main_dir': '', 'model_dir': '', 'model_type': 'nac', 'pyr_version': None}, 'model': {'Depth': 3, 'activ': 'leaky_softplus', 'activ_alpha': 0.05, 'angle_index': [], 'angle_mean': 0, 'angle_std': 1, 'atoms': 2, 'dihyd_index': [], 'dihyd_mean': 0, 'dihyd_std': 1, 'dropout': 0.005, 'invd_index': [], 'invd_mean': 0, 'invd_std': 1, 'learning_rate_compile': 0.001, 'nn_size': 1000, 'phase_less_loss': True, 'reg_l1': 1e-05, 'reg_l2': 1e-05, 'states': 1, 'use_bond_angles': False, 'use_dihyd_angles': False, 'use_dropout': False, 'use_invdist': True, 'use_reg_activ': None, 'use_reg_bias': None, 'use_reg_weight': None, 'y_nac_mean': 0, 'y_nac_std': 1, 'y_nac_unit_conv': 1.8897261246229133}, 'plots': {'unit_nac': '1/A'}, 'predict': {'batch_size_predict': 265, 'try_predict_hessian': False}, 'resample': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'phase_less_loss': True, 'pre_epo': 100, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': False, 'val_split': 0.05}, 'retraining': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 2000, 'epoch_step_reduction': [1000, 500, 250], 'epomin': 1000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'phase_less_loss': True, 'pre_epo': 100, 'reinit_weights': False, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}, 'training': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'pre_epo': 100, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_nac.create_model_nac" title="Permalink to this definition">¶</a></dt>
<dd><p>Get full model with potential = model(x) with coordinates x. The gradients are computed
by predict_step() or seperately in tf.GradientTape(). See subclassed NACModel().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyper</strong> (<em>dict</em><em>, </em><em>optional</em>) – Hyper dictionary. The default is hyper_create_model_nac.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>model</strong> – tf.keras model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.keras.model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.models_nac.create_model_nac_precomputed">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.models_nac.</code><code class="sig-name descname">create_model_nac_precomputed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hyper</span><span class="o">=</span><span class="default_value">{'general': {'info': '', 'main_dir': '', 'model_dir': '', 'model_type': 'nac', 'pyr_version': None}, 'model': {'Depth': 3, 'activ': 'leaky_softplus', 'activ_alpha': 0.05, 'angle_index': [], 'angle_mean': 0, 'angle_std': 1, 'atoms': 2, 'dihyd_index': [], 'dihyd_mean': 0, 'dihyd_std': 1, 'dropout': 0.005, 'invd_index': [], 'invd_mean': 0, 'invd_std': 1, 'learning_rate_compile': 0.001, 'nn_size': 1000, 'phase_less_loss': True, 'reg_l1': 1e-05, 'reg_l2': 1e-05, 'states': 1, 'use_bond_angles': False, 'use_dihyd_angles': False, 'use_dropout': False, 'use_invdist': True, 'use_reg_activ': None, 'use_reg_bias': None, 'use_reg_weight': None, 'y_nac_mean': 0, 'y_nac_std': 1, 'y_nac_unit_conv': 1.8897261246229133}, 'plots': {'unit_nac': '1/A'}, 'predict': {'batch_size_predict': 265, 'try_predict_hessian': False}, 'resample': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'phase_less_loss': True, 'pre_epo': 100, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': False, 'val_split': 0.05}, 'retraining': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 2000, 'epoch_step_reduction': [1000, 500, 250], 'epomin': 1000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'phase_less_loss': True, 'pre_epo': 100, 'reinit_weights': False, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}, 'training': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'pre_epo': 100, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}}</span></em>, <em class="sig-param"><span class="n">force_phase_loss</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.models_nac.create_model_nac_precomputed" title="Permalink to this definition">¶</a></dt>
<dd><p>Get precomputed withmodel y = model(feat) with feat=[f,df/dx] features
and its derivative to coordinates x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hyper</strong> (<em>dict</em><em>, </em><em>optional</em>) – Hyper dictionary. The default is hyper_create_model_nac.</p></li>
<li><p><strong>force_phase_loss</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use normal loss MSE regardless of hyper. The default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>model</strong> – tf.keras model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.keras.model</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.oracle">
<span id="pynnsmd-nn-pes-src-oracle-module"></span><h2>pyNNsMD.nn_pes_src.oracle module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.oracle" title="Permalink to this headline">¶</a></h2>
<p>Methods for active sampling.</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.oracle.find_samples_with_max_error">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.oracle.</code><code class="sig-name descname">find_samples_with_max_error</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">y_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.oracle.find_samples_with_max_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the sample indices with the combined maximum deviation for all model in y.
y_pred may have more models. Each np.array in y and y_pred must have same size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>dict</em>) – Dictionary of y values by model key.</p></li>
<li><p><strong>y_pred</strong> (<em>dict</em>) – Dictionary of predicted values by model key.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>out</strong> (<em>np.array</em>) – Sorted index array.</p></li>
<li><p><strong>err_total_dict</strong> (<em>dict</em>) – Dictionary of mean absolute error that matches structure of y.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.plot">
<span id="pynnsmd-nn-pes-src-plot-module"></span><h2>pyNNsMD.nn_pes_src.plot module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.plot" title="Permalink to this headline">¶</a></h2>
<p>Functions to plot fitresults.</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.plot.find_max_relative_error">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.plot.</code><code class="sig-name descname">find_max_relative_error</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">preds</span></em>, <em class="sig-param"><span class="n">yval</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.plot.find_max_relative_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Find maximum error and its relative value if possible.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>preds</strong> (<em>np.array</em>) – Prediction array.</p></li>
<li><p><strong>yval</strong> (<em>np.array</em>) – Validation array.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>pred_err</strong> (<em>np.array</em>) – Flatten maximum error along axis=0</p></li>
<li><p><strong>prelm</strong> (<em>np.array</em>) – Flatten Relative maximum error along axis=0</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.plot.plot_energy_gradient_fit_result">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.plot.</code><code class="sig-name descname">plot_energy_gradient_fit_result</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">xval</span></em>, <em class="sig-param"><span class="n">xtrain</span></em>, <em class="sig-param"><span class="n">yval</span></em>, <em class="sig-param"><span class="n">ytrain</span></em>, <em class="sig-param"><span class="n">predval</span></em>, <em class="sig-param"><span class="n">predtrain</span></em>, <em class="sig-param"><span class="n">hist</span></em>, <em class="sig-param"><span class="n">epostep</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">dir_save</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">unit_energy</span><span class="o">=</span><span class="default_value">'eV'</span></em>, <em class="sig-param"><span class="n">unit_force</span><span class="o">=</span><span class="default_value">'eV/A'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.plot.plot_energy_gradient_fit_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot and store fit</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>i</strong> (<em>int</em>) – index of the nerual network.</p></li>
<li><p><strong>xval</strong> (<em>np.array</em>) – Validation Data.</p></li>
<li><p><strong>xtrain</strong> (<em>np.array</em>) – Training Data.</p></li>
<li><p><strong>yval</strong> (<em>list</em>) – True Validation data. [energy, gradient]</p></li>
<li><p><strong>ytrain</strong> (<em>list</em>) – True Training data. [energy, gradient]</p></li>
<li><p><strong>predval</strong> (<em>list</em>) – Model prediction for validation.</p></li>
<li><p><strong>predtrain</strong> (<em>list</em>) – Model prediction for training.</p></li>
<li><p><strong>hist</strong> (<em>dict</em>) – histogram from keras.</p></li>
<li><p><strong>epostep</strong> (<em>int</em><em>, </em><em>optional</em>) – Step size of evals. The default is 1.</p></li>
<li><p><strong>dir_save</strong> (<em>str/os.path</em><em>, </em><em>optional</em>) – Path of dictionary to store plots. The default is None.</p></li>
<li><p><strong>unit_energy</strong> (<em>str</em><em>, </em><em>optional</em>) – Unit of energy. The default is ‘eV’.</p></li>
<li><p><strong>unit_force</strong> (<em>str</em><em>, </em><em>optional</em>) – Unit of force. The default is ‘eV/A’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ol class="arabic simple" start="0">
<li></li>
</ol>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.plot.plot_nac_fit_result">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.plot.</code><code class="sig-name descname">plot_nac_fit_result</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">xval</span></em>, <em class="sig-param"><span class="n">xtrain</span></em>, <em class="sig-param"><span class="n">yval</span></em>, <em class="sig-param"><span class="n">ytrain</span></em>, <em class="sig-param"><span class="n">predval</span></em>, <em class="sig-param"><span class="n">predtrain</span></em>, <em class="sig-param"><span class="n">hist</span></em>, <em class="sig-param"><span class="n">epostep</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">dir_save</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">prename</span><span class="o">=</span><span class="default_value">'fit'</span></em>, <em class="sig-param"><span class="n">unit_nac</span><span class="o">=</span><span class="default_value">'1/A'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.plot.plot_nac_fit_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot and store fit</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>i</strong> (<em>int</em>) – index of the nerual network.</p></li>
<li><p><strong>xval</strong> (<em>np.array</em>) – Validation Data.</p></li>
<li><p><strong>xtrain</strong> (<em>np.array</em>) – Training Data.</p></li>
<li><p><strong>yval</strong> (<em>np.array</em>) – True Validation data. nac</p></li>
<li><p><strong>ytrain</strong> (<em>np.array</em>) – True Training data. nac</p></li>
<li><p><strong>predval</strong> (<em>np.array</em>) – Model prediction for validation.</p></li>
<li><p><strong>predtrain</strong> (<em>np.array</em>) – Model prediction for training.</p></li>
<li><p><strong>hist</strong> (<em>dict</em>) – histogram from keras.</p></li>
<li><p><strong>epostep</strong> (<em>int</em><em>, </em><em>optional</em>) – Step size of evals. The default is 1.</p></li>
<li><p><strong>dir_save</strong> (<em>str/os.path</em><em>, </em><em>optional</em>) – Path of dictionary to store plots. The default is None.</p></li>
<li><p><strong>prename</strong> (<em>str</em><em>, </em><em>optional</em>) – Start of naming plots. The default is ‘fit’.</p></li>
<li><p><strong>unit_nac</strong> (<em>str</em><em>, </em><em>optional</em>) – Unit of NACs. The default is ‘1/A’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ol class="arabic simple" start="0">
<li></li>
</ol>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.plot.plot_resampling_gradient">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.plot.</code><code class="sig-name descname">plot_resampling_gradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dir_save</span></em>, <em class="sig-param"><span class="n">out_index</span></em>, <em class="sig-param"><span class="n">pool_error</span></em>, <em class="sig-param"><span class="n">fit_error</span></em>, <em class="sig-param"><span class="n">test_error</span></em>, <em class="sig-param"><span class="n">prename</span><span class="o">=</span><span class="default_value">'resample'</span></em>, <em class="sig-param"><span class="n">unit_energy_conv</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">unit_force_conv</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">unit_energy</span><span class="o">=</span><span class="default_value">'eV'</span></em>, <em class="sig-param"><span class="n">unit_force</span><span class="o">=</span><span class="default_value">'eV/A'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.plot.plot_resampling_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot the resampling statistics for energy gradient</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dir_save</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory to save data. The default is None.</p></li>
<li><p><strong>out_index</strong> (<em>list</em><em>,</em>) – List of indexarrays from data.</p></li>
<li><p><strong>pool_error</strong> (<em>np.array</em><em>,</em>) – List of error on the remaining data.</p></li>
<li><p><strong>fit_error</strong> (<em>np.array</em><em>,</em>) – List of error of validation set.</p></li>
<li><p><strong>test_error</strong> (<em>np.array</em><em>,</em>) – List of error for test set.</p></li>
<li><p><strong>prename</strong> (<em>str</em><em>, </em><em>optional</em>) – Prefix for fit. The default is ‘fit’.</p></li>
<li><p><strong>unit_energy_conv</strong> (<em>float</em><em>, </em><em>optional</em>) – Value of unitconverion for energy. The default is 1.</p></li>
<li><p><strong>unit_force_conv</strong> (<em>float</em><em>, </em><em>optional</em>) – Value of unitconversion for force. The default is 1.</p></li>
<li><p><strong>unit_energy</strong> (<em>str</em><em>, </em><em>optional</em>) – Unit of energy. The default is ‘eV’.</p></li>
<li><p><strong>unit_force</strong> (<em>str</em><em>, </em><em>optional</em>) – Unit of force. The default is ‘eV/A’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.plot.plot_resampling_nac">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.plot.</code><code class="sig-name descname">plot_resampling_nac</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dir_save</span></em>, <em class="sig-param"><span class="n">out_index</span></em>, <em class="sig-param"><span class="n">pool_error</span></em>, <em class="sig-param"><span class="n">fit_error</span></em>, <em class="sig-param"><span class="n">test_error</span></em>, <em class="sig-param"><span class="n">prename</span><span class="o">=</span><span class="default_value">'resample'</span></em>, <em class="sig-param"><span class="n">unit_nac_conv</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">unit_nac</span><span class="o">=</span><span class="default_value">'1/A'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.plot.plot_resampling_nac" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot the resampling statistics for energy gradient</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dir_save</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory to save data. The default is None.</p></li>
<li><p><strong>out_index</strong> (<em>list</em><em>,</em>) – List of indexarrays from data.</p></li>
<li><p><strong>pool_error</strong> (<em>np.array</em><em>,</em>) – List of error on the remaining data.</p></li>
<li><p><strong>fit_error</strong> (<em>np.array</em><em>,</em>) – List of error of validation set.</p></li>
<li><p><strong>test_error</strong> (<em>np.array</em><em>,</em>) – List of error for test set.</p></li>
<li><p><strong>prename</strong> (<em>str</em><em>, </em><em>optional</em>) – Prefix for fit. The default is ‘fit’.</p></li>
<li><p><strong>unit_nac_conv</strong> (<em>float</em><em>, </em><em>optional</em>) – Value of unitconversion for nac. The default is 1.</p></li>
<li><p><strong>unit_nac</strong> (<em>str</em><em>, </em><em>optional</em>) – Unit of NAC. The default is ‘1/A’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.predict">
<span id="pynnsmd-nn-pes-src-predict-module"></span><h2>pyNNsMD.nn_pes_src.predict module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.predict" title="Permalink to this headline">¶</a></h2>
<p>Functions for Model prediction that handles scaling for precomputed models.</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.predict.call_model_energy_gradient_precomputed">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.predict.</code><code class="sig-name descname">call_model_energy_gradient_precomputed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ml</span></em>, <em class="sig-param"><span class="n">mf</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">hyper</span><span class="o">=</span><span class="default_value">{'general': {'info': '', 'main_dir': '', 'model_dir': '', 'model_type': 'energy_gradient', 'pyr_version': None}, 'model': {'Depth': 3, 'activ': 'leaky_softplus', 'activ_alpha': 0.03, 'angle_index': [], 'angle_mean': 0, 'angle_std': 1, 'atoms': 2, 'dihyd_index': [], 'dihyd_mean': 0, 'dihyd_std': 1, 'dropout': 0.005, 'invd_index': [], 'invd_mean': 0, 'invd_std': 1, 'learning_rate_compile': 0.001, 'loss_weights': [1, 10], 'nn_size': 1000, 'reg_l1': 1e-05, 'reg_l2': 1e-05, 'states': 1, 'use_bond_angles': False, 'use_dihyd_angles': False, 'use_dropout': False, 'use_invdist': True, 'use_reg_activ': None, 'use_reg_bias': None, 'use_reg_weight': None, 'y_energy_mean': 0, 'y_energy_std': 1, 'y_energy_unit_conv': 27.21138624598853, 'y_gradient_unit_conv': 51.42206747624915}, 'plots': {'unit_energy': 'eV', 'unit_gradient': 'eV/A'}, 'predict': {'batch_size_predict': 265, 'try_predict_hessian': False}, 'resample': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': False, 'val_split': 0.05}, 'retraining': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 2000, 'epoch_step_reduction': [1000, 500, 250], 'epomin': 1000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': False, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}, 'training': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.predict.call_model_energy_gradient_precomputed" title="Permalink to this definition">¶</a></dt>
<dd><p>Call energy plus gradient from separate FeatureModel and EnergyModelPrecomputed. Scales to original y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ml</strong> (<em>tf.keras.model</em>) – Model for energy.</p></li>
<li><p><strong>mf</strong> (<em>tf.keras.model</em>) – Model for features.</p></li>
<li><p><strong>x</strong> (<em>tf.tensor</em>) – Coordinates in shape (batch,Atoms,3).</p></li>
<li><p><strong>hyper</strong> (<em>dict</em><em>, </em><em>optional</em>) – Hyperparameter dictionary. The default is hyper_predict_model_energy_gradient.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>temp_e</strong> (<em>tf.tensor</em>) – Calculated and rescaled energy.</p></li>
<li><p><strong>temp_g</strong> (<em>tf.tensor</em>) – Calculated and rescaled gradient.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.predict.call_model_nac_precomputed">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.predict.</code><code class="sig-name descname">call_model_nac_precomputed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ml</span></em>, <em class="sig-param"><span class="n">mf</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">hyper</span><span class="o">=</span><span class="default_value">{'general': {'info': '', 'main_dir': '', 'model_dir': '', 'model_type': 'nac', 'pyr_version': None}, 'model': {'Depth': 3, 'activ': 'leaky_softplus', 'activ_alpha': 0.05, 'angle_index': [], 'angle_mean': 0, 'angle_std': 1, 'atoms': 2, 'dihyd_index': [], 'dihyd_mean': 0, 'dihyd_std': 1, 'dropout': 0.005, 'invd_index': [], 'invd_mean': 0, 'invd_std': 1, 'learning_rate_compile': 0.001, 'nn_size': 1000, 'phase_less_loss': True, 'reg_l1': 1e-05, 'reg_l2': 1e-05, 'states': 1, 'use_bond_angles': False, 'use_dihyd_angles': False, 'use_dropout': False, 'use_invdist': True, 'use_reg_activ': None, 'use_reg_bias': None, 'use_reg_weight': None, 'y_nac_mean': 0, 'y_nac_std': 1, 'y_nac_unit_conv': 1.8897261246229133}, 'plots': {'unit_nac': '1/A'}, 'predict': {'batch_size_predict': 265, 'try_predict_hessian': False}, 'resample': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'phase_less_loss': True, 'pre_epo': 100, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': False, 'val_split': 0.05}, 'retraining': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 2000, 'epoch_step_reduction': [1000, 500, 250], 'epomin': 1000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'phase_less_loss': True, 'pre_epo': 100, 'reinit_weights': False, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}, 'training': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'pre_epo': 100, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.predict.call_model_nac_precomputed" title="Permalink to this definition">¶</a></dt>
<dd><p>Call NAC from separate FeatureModel and NACModelPrecomputed. Scales to original y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ml</strong> (<em>tf.keras.model</em>) – Model for NAC virutal potential.</p></li>
<li><p><strong>mf</strong> (<em>tf.keras.model</em>) – Model for features.</p></li>
<li><p><strong>x</strong> (<em>tf.tensor</em>) – Coordinates in shape (batch,Atoms,3).</p></li>
<li><p><strong>hyper</strong> (<em>dict</em><em>, </em><em>optional</em>) – Hyperparameter dictionary. The default is hyper_predict_model_nac.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>temp</strong> – Calculated and rescaled NAC.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.predict.predict_model_energy_gradient">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.predict.</code><code class="sig-name descname">predict_model_energy_gradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ml</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">batch_size_predict</span><span class="o">=</span><span class="default_value">32</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.predict.predict_model_energy_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict energy plus gradient from EnergyModel. Scales to original y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ml</strong> (<em>tf.keras.model</em>) – Model for energy.</p></li>
<li><p><strong>x</strong> (<em>np.array</em>) – Coordinates in shape (batch,Atoms,3).</p></li>
<li><p><strong>batch_size_predict</strong> (<em>int</em><em>, </em><em>optional</em>) – The batch size used in prediction.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>temp_e</strong> (<em>np.array</em>) – Calculated and rescaled energy.</p></li>
<li><p><strong>temp_g</strong> (<em>np.arry</em>) – Calculated and rescaled gradient.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.predict.predict_model_energy_gradient_precomputed">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.predict.</code><code class="sig-name descname">predict_model_energy_gradient_precomputed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ml</span></em>, <em class="sig-param"><span class="n">mf</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">hyper</span><span class="o">=</span><span class="default_value">{'general': {'info': '', 'main_dir': '', 'model_dir': '', 'model_type': 'energy_gradient', 'pyr_version': None}, 'model': {'Depth': 3, 'activ': 'leaky_softplus', 'activ_alpha': 0.03, 'angle_index': [], 'angle_mean': 0, 'angle_std': 1, 'atoms': 2, 'dihyd_index': [], 'dihyd_mean': 0, 'dihyd_std': 1, 'dropout': 0.005, 'invd_index': [], 'invd_mean': 0, 'invd_std': 1, 'learning_rate_compile': 0.001, 'loss_weights': [1, 10], 'nn_size': 1000, 'reg_l1': 1e-05, 'reg_l2': 1e-05, 'states': 1, 'use_bond_angles': False, 'use_dihyd_angles': False, 'use_dropout': False, 'use_invdist': True, 'use_reg_activ': None, 'use_reg_bias': None, 'use_reg_weight': None, 'y_energy_mean': 0, 'y_energy_std': 1, 'y_energy_unit_conv': 27.21138624598853, 'y_gradient_unit_conv': 51.42206747624915}, 'plots': {'unit_energy': 'eV', 'unit_gradient': 'eV/A'}, 'predict': {'batch_size_predict': 265, 'try_predict_hessian': False}, 'resample': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': False, 'val_split': 0.05}, 'retraining': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 2000, 'epoch_step_reduction': [1000, 500, 250], 'epomin': 1000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': False, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}, 'training': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.predict.predict_model_energy_gradient_precomputed" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict energy plus gradient from separate FeatureModel and EnergyModelPrecomputed. Scales to original y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ml</strong> (<em>tf.keras.model</em>) – Model that accepts features and returns energy.</p></li>
<li><p><strong>mf</strong> (<em>tf.keras.model</em>) – Model for features.</p></li>
<li><p><strong>x</strong> (<em>np.array</em>) – Coordinates in shape (batch,Atoms,3).</p></li>
<li><p><strong>hyper</strong> (<em>dict</em><em>, </em><em>optional</em>) – Hyperparameter dictionary. The default is hyper_predict_model_energy_gradient.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>temp_e</strong> (<em>np.array</em>) – Calculated and scaled energy.</p></li>
<li><p><strong>temp_g</strong> (<em>np.arry</em>) – Calculated and scaled gradient.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.predict.predict_model_nac">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.predict.</code><code class="sig-name descname">predict_model_nac</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ml</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">batch_size_predict</span><span class="o">=</span><span class="default_value">32</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.predict.predict_model_nac" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict NAC. Scales to original y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ml</strong> (<em>tf.keras.model</em>) – Model for NAC virutal potential.</p></li>
<li><p><strong>x</strong> (<em>np.array</em>) – Coordinates in shape (batch,Atoms,3).</p></li>
<li><p><strong>batch_size_predict</strong> (<em>int</em><em>, </em><em>optional</em>) – The batch size used in prediction.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>temp</strong> – Calculated and rescaled NAC.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.predict.predict_model_nac_precomputed">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.predict.</code><code class="sig-name descname">predict_model_nac_precomputed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ml</span></em>, <em class="sig-param"><span class="n">mf</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">hyper</span><span class="o">=</span><span class="default_value">{'general': {'info': '', 'main_dir': '', 'model_dir': '', 'model_type': 'nac', 'pyr_version': None}, 'model': {'Depth': 3, 'activ': 'leaky_softplus', 'activ_alpha': 0.05, 'angle_index': [], 'angle_mean': 0, 'angle_std': 1, 'atoms': 2, 'dihyd_index': [], 'dihyd_mean': 0, 'dihyd_std': 1, 'dropout': 0.005, 'invd_index': [], 'invd_mean': 0, 'invd_std': 1, 'learning_rate_compile': 0.001, 'nn_size': 1000, 'phase_less_loss': True, 'reg_l1': 1e-05, 'reg_l2': 1e-05, 'states': 1, 'use_bond_angles': False, 'use_dihyd_angles': False, 'use_dropout': False, 'use_invdist': True, 'use_reg_activ': None, 'use_reg_bias': None, 'use_reg_weight': None, 'y_nac_mean': 0, 'y_nac_std': 1, 'y_nac_unit_conv': 1.8897261246229133}, 'plots': {'unit_nac': '1/A'}, 'predict': {'batch_size_predict': 265, 'try_predict_hessian': False}, 'resample': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'phase_less_loss': True, 'pre_epo': 100, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': False, 'val_split': 0.05}, 'retraining': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 2000, 'epoch_step_reduction': [1000, 500, 250], 'epomin': 1000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'phase_less_loss': True, 'pre_epo': 100, 'reinit_weights': False, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}, 'training': {'batch_size': 64, 'delta_loss': 1e-05, 'epo': 10000, 'epoch_step_reduction': [4000, 3000, 2000, 1000], 'epomin': 5000, 'epostep': 10, 'factor_lr': 0.1, 'learning_rate_start': 0.001, 'learning_rate_step': [0.001, 0.0001, 1e-05, 1e-06], 'learning_rate_stop': 1e-06, 'loss_monitor': 'val_loss', 'max_time': 600, 'patience': 600, 'pre_epo': 100, 'reinit_weights': True, 'use_early_callback': False, 'use_exp_callback': False, 'use_linear_callback': False, 'use_step_callback': True, 'val_disjoint': True, 'val_split': 0.1}}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.predict.predict_model_nac_precomputed" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict NAC from separate FeatureModel and NACModelPrecomputed. Scales to original y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ml</strong> (<em>tf.keras.model</em>) – Model that accepts features and returns NAC virutal potential.</p></li>
<li><p><strong>mf</strong> (<em>tf.keras.model</em>) – Model for features.</p></li>
<li><p><strong>x</strong> (<em>np.array</em>) – Coordinates in shape (batch,Atoms,3).</p></li>
<li><p><strong>hyper</strong> (<em>dict</em><em>, </em><em>optional</em>) – Hyperparameter dictionary. The default is hyper_predict_model_nac.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>temp</strong> – Calculated and rescaled NAC.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pyNNsMD.nn_pes_src.reg">
<span id="pynnsmd-nn-pes-src-reg-module"></span><h2>pyNNsMD.nn_pes_src.reg module<a class="headerlink" href="#module-pyNNsMD.nn_pes_src.reg" title="Permalink to this headline">¶</a></h2>
<p>Define forms of regularization</p>
<dl class="py function">
<dt id="pyNNsMD.nn_pes_src.reg.identify_regularizer">
<code class="sig-prename descclassname">pyNNsMD.nn_pes_src.reg.</code><code class="sig-name descname">identify_regularizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">reg_str</span></em>, <em class="sig-param"><span class="n">l1_str</span></em>, <em class="sig-param"><span class="n">l2_str</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyNNsMD.nn_pes_src.reg.identify_regularizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify regularization. This is redundant and will be replaced by tf.keras.regularizatio.get().</p>
</dd></dl>

</div>
<div class="section" id="pynnsmd-nn-pes-src-training-eg-module">
<h2>pyNNsMD.nn_pes_src.training_eg module<a class="headerlink" href="#pynnsmd-nn-pes-src-training-eg-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="pynnsmd-nn-pes-src-training-nac-module">
<h2>pyNNsMD.nn_pes_src.training_nac module<a class="headerlink" href="#pynnsmd-nn-pes-src-training-nac-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-pyNNsMD.nn_pes_src">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pyNNsMD.nn_pes_src" title="Permalink to this headline">¶</a></h2>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">pyNNsMD.nn_pes_src package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.activ">pyNNsMD.nn_pes_src.activ module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.callbacks">pyNNsMD.nn_pes_src.callbacks module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.data">pyNNsMD.nn_pes_src.data module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.device">pyNNsMD.nn_pes_src.device module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.fit">pyNNsMD.nn_pes_src.fit module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.hyper">pyNNsMD.nn_pes_src.hyper module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.layers">pyNNsMD.nn_pes_src.layers module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.legacy">pyNNsMD.nn_pes_src.legacy module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.loss">pyNNsMD.nn_pes_src.loss module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.models_eg">pyNNsMD.nn_pes_src.models_eg module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.models_feat">pyNNsMD.nn_pes_src.models_feat module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.models_nac">pyNNsMD.nn_pes_src.models_nac module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.oracle">pyNNsMD.nn_pes_src.oracle module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.plot">pyNNsMD.nn_pes_src.plot module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.predict">pyNNsMD.nn_pes_src.predict module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src.reg">pyNNsMD.nn_pes_src.reg module</a></li>
<li><a class="reference internal" href="#pynnsmd-nn-pes-src-training-eg-module">pyNNsMD.nn_pes_src.training_eg module</a></li>
<li><a class="reference internal" href="#pynnsmd-nn-pes-src-training-nac-module">pyNNsMD.nn_pes_src.training_nac module</a></li>
<li><a class="reference internal" href="#module-pyNNsMD.nn_pes_src">Module contents</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="pyNNsMD.html"
                        title="previous chapter">pyNNsMD package</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/pyNNsMD.nn_pes_src.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="pyNNsMD.html" title="pyNNsMD package"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">NNsForMD 0.5.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="modules.html" >pyNNsMD</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="pyNNsMD.html" >pyNNsMD package</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">pyNNsMD.nn_pes_src package</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Patrick.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.2.1.
    </div>
  </body>
</html>